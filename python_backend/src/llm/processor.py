"""
LLM å¤„ç†å™¨ - Pythonç‰ˆæœ¬
æ”¯æŒå¤šç§ LLM æä¾›å•†ï¼Œé›†æˆ MCP å·¥å…·è°ƒç”¨åŠŸèƒ½
"""

import json
import time
from typing import Dict, List, Optional, Any
from loguru import logger
import openai
from tenacity import retry, stop_after_attempt, wait_exponential

from ..mcp.types import (
    LLMConfig, ChatMessage, ProcessResult, FunctionCall, FunctionCallResult,
    MCPException
)
from ..mcp.client import MCPClient


class EnhancedLLMProcessor:
    """å¢å¼ºç‰ˆ LLM å¤„ç†å™¨ï¼Œæ”¯æŒ MCP å·¥å…·è°ƒç”¨"""
    
    def __init__(self, llm_config: LLMConfig, mcp_client: MCPClient):
        self.config = llm_config
        self.mcp_client = mcp_client
        self.client = self._initialize_client()
        
    def _initialize_client(self):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        if self.config.provider == "openai":
            return openai.OpenAI(
                api_key=self.config.api_key,
                base_url=self.config.base_url
            )
        else:
            # æ”¯æŒå…¶ä»–æä¾›å•†
            raise NotImplementedError(f"Provider {self.config.provider} not implemented yet")
    
    async def chat(
        self,
        messages: List[ChatMessage],
        enable_tools: bool = False
    ) -> ProcessResult:
        """æ™®é€šèŠå¤©å¤„ç†"""
        try:
            if enable_tools and self.mcp_client.status.value == "connected":
                return await self._chat_with_tools(messages)
            else:
                return await self._chat_without_tools(messages)
        except Exception as e:
            logger.error(f"LLM å¤„ç†å¤±è´¥: {e}")
            raise MCPException("LLM_PROCESSING_FAILED", "LLM processing failed", str(e))
    
    async def chat_with_shortcuts(
        self,
        shortcut: str,
        content: str,
        context: Optional[Dict[str, Any]] = None
    ) -> ProcessResult:
        """å¿«æ·æŒ‡ä»¤å¤„ç†"""
        shortcut_prompts = {
            "/pods": "è¯·è·å–Kubernetesé›†ç¾¤ä¸­çš„Podåˆ—è¡¨ï¼Œå¹¶ä»¥æ˜“è¯»çš„æ ¼å¼å±•ç¤º",
            "/logs": "è¯·è·å–æŒ‡å®šPodçš„æœ€æ–°æ—¥å¿—",
            "/scale": "è¯·æ‰©ç¼©å®¹æŒ‡å®šçš„Deployment",
            "/status": "è¯·æ£€æŸ¥é›†ç¾¤çŠ¶æ€å’Œå¥åº·æƒ…å†µ",
            "/help": "æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨çš„å¿«æ·æŒ‡ä»¤"
        }
        
        prompt = shortcut_prompts.get(shortcut)
        if not prompt:
            return ProcessResult(
                content=f"æœªçŸ¥çš„å¿«æ·æŒ‡ä»¤: {shortcut}\n\nå¯ç”¨æŒ‡ä»¤:\n" + 
                       "\n".join(f"- {k}: {v}" for k, v in shortcut_prompts.items())
            )
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            ChatMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Kubernetesè¿ç»´åŠ©æ‰‹ï¼Œæ“…é•¿ä½¿ç”¨K8så·¥å…·æ¥ç®¡ç†é›†ç¾¤ã€‚"),
            ChatMessage(role="user", content=f"{prompt}\n\nç”¨æˆ·è¡¥å……ä¿¡æ¯: {content}")
        ]
        
        return await self.chat(messages, enable_tools=True)
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=60))
    async def _chat_without_tools(self, messages: List[ChatMessage]) -> ProcessResult:
        """ä¸ä½¿ç”¨å·¥å…·çš„èŠå¤©"""
        openai_messages = self._convert_messages_to_openai(messages)
        
        response = await self.client.chat.completions.acreate(
            model=self.config.model,
            messages=openai_messages,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens
        )
        
        return ProcessResult(
            content=response.choices[0].message.content,
            usage=response.usage.model_dump() if response.usage else None
        )
    
    async def _chat_with_tools(self, messages: List[ChatMessage]) -> ProcessResult:
        """ä½¿ç”¨å·¥å…·çš„èŠå¤©"""
        # è·å–å¯ç”¨å·¥å…·
        tools = await self.mcp_client.list_tools()
        if not tools:
            return await self._chat_without_tools(messages)
        
        # è½¬æ¢å·¥å…·ä¸º OpenAI æ ¼å¼
        openai_tools = self._convert_tools_to_openai(tools)
        openai_messages = self._convert_messages_to_openai(messages)
        
        # è°ƒç”¨ LLM
        response = await self.client.chat.completions.acreate(
            model=self.config.model,
            messages=openai_messages,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            tools=openai_tools,
            tool_choice="auto"
        )
        
        message = response.choices[0].message
        
        # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›
        if not message.tool_calls:
            return ProcessResult(
                content=message.content,
                usage=response.usage.model_dump() if response.usage else None
            )
        
        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        function_results = []
        for tool_call in message.tool_calls:
            try:
                # è§£æå‚æ•°
                parameters = json.loads(tool_call.function.arguments)
                
                # è°ƒç”¨ MCP å·¥å…·
                result = await self.mcp_client.call_tool(
                    tool_call.function.name,
                    parameters
                )
                
                function_results.append(FunctionCallResult(
                    function_call=FunctionCall(
                        name=tool_call.function.name,
                        arguments=tool_call.function.arguments
                    ),
                    result=result
                ))
                
                # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
                openai_messages.append({
                    "role": "assistant",
                    "content": message.content,
                    "tool_calls": [{
                        "id": tool_call.id,
                        "type": "function",
                        "function": {
                            "name": tool_call.function.name,
                            "arguments": tool_call.function.arguments
                        }
                    }]
                })
                
                openai_messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": json.dumps(result, ensure_ascii=False, indent=2)
                })
                
            except Exception as e:
                logger.error(f"å·¥å…·è°ƒç”¨å¤±è´¥ {tool_call.function.name}: {e}")
                function_results.append(FunctionCallResult(
                    function_call=FunctionCall(
                        name=tool_call.function.name,
                        arguments=tool_call.function.arguments
                    ),
                    error=str(e)
                ))
        
        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ç»“æœï¼Œå†æ¬¡è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå›å¤
        if function_results:
            final_response = await self.client.chat.completions.acreate(
                model=self.config.model,
                messages=openai_messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )
            
            final_content = self._format_response_with_tools(
                final_response.choices[0].message.content,
                function_results
            )
            
            return ProcessResult(
                content=final_content,
                function_calls=function_results,
                usage=final_response.usage.model_dump() if final_response.usage else None
            )
        
        return ProcessResult(
            content=message.content or "æ‰§è¡Œå®Œæˆ",
            function_calls=function_results,
            usage=response.usage.model_dump() if response.usage else None
        )
    
    def _convert_messages_to_openai(self, messages: List[ChatMessage]) -> List[Dict[str, Any]]:
        """è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸º OpenAI æ ¼å¼"""
        result = []
        for msg in messages:
            openai_msg = {
                "role": msg.role,
                "content": msg.content
            }
            
            if msg.tool_call_id:
                openai_msg["tool_call_id"] = msg.tool_call_id
                
            if msg.function_call:
                openai_msg["function_call"] = {
                    "name": msg.function_call.name,
                    "arguments": msg.function_call.arguments
                }
            
            result.append(openai_msg)
        
        return result
    
    def _convert_tools_to_openai(self, tools) -> List[Dict[str, Any]]:
        """è½¬æ¢ MCP å·¥å…·ä¸º OpenAI å·¥å…·æ ¼å¼"""
        result = []
        for tool in tools:
            result.append({
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.input_schema
                }
            })
        return result
    
    def _format_response_with_tools(
        self,
        content: str,
        function_results: List[FunctionCallResult]
    ) -> str:
        """æ ¼å¼åŒ–åŒ…å«å·¥å…·è°ƒç”¨ç»“æœçš„å“åº”"""
        if not function_results:
            return content
        
        formatted_content = content + "\n\n**å·¥å…·è°ƒç”¨è¯¦æƒ…:**\n"
        
        for i, result in enumerate(function_results, 1):
            formatted_content += f"\n**{i}. {result.function_call.name}**\n"
            
            if result.error:
                formatted_content += f"âŒ æ‰§è¡Œå¤±è´¥: {result.error}\n"
            else:
                formatted_content += f"âœ… æ‰§è¡ŒæˆåŠŸ\n"
                # ç®€åŒ–ç»“æœæ˜¾ç¤º
                if isinstance(result.result, dict):
                    if "items" in result.result:
                        formatted_content += f"ğŸ“Š è¿”å› {len(result.result['items'])} é¡¹ç»“æœ\n"
                    else:
                        formatted_content += f"ğŸ“‹ ç»“æœ: {json.dumps(result.result, ensure_ascii=False, indent=2)[:200]}...\n"
                else:
                    formatted_content += f"ğŸ“‹ ç»“æœ: {str(result.result)[:200]}...\n"
        
        return formatted_content
    
    async def get_available_shortcuts(self) -> Dict[str, str]:
        """è·å–å¯ç”¨çš„å¿«æ·æŒ‡ä»¤"""
        shortcuts = {
            "/pods": "æŸ¥çœ‹Podåˆ—è¡¨",
            "/logs": "æŸ¥çœ‹Podæ—¥å¿—",
            "/scale": "æ‰©ç¼©å®¹Deployment", 
            "/status": "æ£€æŸ¥é›†ç¾¤çŠ¶æ€",
            "/help": "æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"
        }
        
        # å¦‚æœ MCP å®¢æˆ·ç«¯è¿æ¥ï¼Œæ·»åŠ å·¥å…·ç›¸å…³çš„å¿«æ·æŒ‡ä»¤
        if self.mcp_client.status.value == "connected":
            tools = await self.mcp_client.list_tools()
            for tool in tools:
                shortcuts[f"/tool-{tool.name}"] = f"ç›´æ¥è°ƒç”¨å·¥å…·: {tool.description}"
        
        return shortcuts
    
    def format_tool_result(self, result: Any) -> str:
        """æ ¼å¼åŒ–å·¥å…·è°ƒç”¨ç»“æœä¸ºç”¨æˆ·å‹å¥½çš„æ–‡æœ¬"""
        if isinstance(result, dict):
            if "items" in result and isinstance(result["items"], list):
                # K8s èµ„æºåˆ—è¡¨æ ¼å¼
                items = result["items"]
                if not items:
                    return "ğŸ“­ æœªæ‰¾åˆ°ä»»ä½•èµ„æº"
                
                formatted = f"ğŸ“¦ æ‰¾åˆ° {len(items)} ä¸ªèµ„æº:\n\n"
                for item in items[:10]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                    metadata = item.get("metadata", {})
                    status = item.get("status", {})
                    formatted += f"â€¢ **{metadata.get('name', 'Unknown')}**\n"
                    formatted += f"  å‘½åç©ºé—´: {metadata.get('namespace', 'default')}\n"
                    formatted += f"  çŠ¶æ€: {status.get('phase', 'Unknown')}\n\n"
                
                if len(items) > 10:
                    formatted += f"... è¿˜æœ‰ {len(items) - 10} ä¸ªèµ„æº\n"
                
                return formatted
            
            elif "pod_name" in result and "content" in result:
                # æ—¥å¿—æ ¼å¼
                return f"ğŸ“‹ **{result['pod_name']}** æ—¥å¿—:\n\n```\n{result['content']}\n```"
            
            elif "deployment_name" in result:
                # æ‰©ç¼©å®¹ç»“æœæ ¼å¼
                return f"ğŸ”„ æ‰©ç¼©å®¹å®Œæˆ:\n" + \
                       f"â€¢ éƒ¨ç½²åç§°: {result['deployment_name']}\n" + \
                       f"â€¢ å‘½åç©ºé—´: {result.get('namespace', 'default')}\n" + \
                       f"â€¢ å‰¯æœ¬æ•°: {result.get('previous_replicas', 0)} â†’ {result.get('target_replicas', 0)}\n" + \
                       f"â€¢ çŠ¶æ€: {'âœ… æˆåŠŸ' if result.get('success') else 'âŒ å¤±è´¥'}"
            
            else:
                # é€šç”¨å­—å…¸æ ¼å¼
                return f"ğŸ“„ ç»“æœ:\n```json\n{json.dumps(result, ensure_ascii=False, indent=2)}\n```"
        
        elif isinstance(result, list):
            return f"ğŸ“‹ åˆ—è¡¨ç»“æœ ({len(result)} é¡¹):\n" + \
                   "\n".join(f"â€¢ {item}" for item in result[:10])
        
        else:
            return f"ï¿½ï¿½ ç»“æœ: {str(result)}" 